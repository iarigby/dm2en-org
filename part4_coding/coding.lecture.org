# -*- mode:org; mode: flyspell; -*-

#+SETUPFILE: ../include/lecture.h.org

#+TITLE: Discrete mathematics II. - Coding
#+SHORT_TITLE: Coding


* Coding theory
** Introduction to information theory
*** A model of communication 
    A in a simplified version of communication, information is
    transferred, from a source to a receiver, trough a (noisy) channel.
    #+BEGIN_EXPORT latex
    \begin{center}
      \begin{tikzpicture}[every node/.style={draw}]
        \draw (0,0) node (src) {Source}; %
        \draw (1.8,0) node (chn) {Channel}; %
        \draw (3.7,0) node (dst) {Receiver}; %
        \draw (1.6,-0.8) node[draw=none,scale=0.7] {A simplified figure of }; %
        \draw [->] (src.east) -- (chn.west);
        \draw [->] (chn.east) -- (dst.west);
      \end{tikzpicture}
    \end{center}
    #+END_EXPORT
    Transfer can happen both in space and time (Netflix vs DVD).
**** Information                                               :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     *Information* is /new/ knowledge. By Shannon, we measure it as the
      amount of reduction of uncertainty.
*** Frequency, self-information
**** Frequency, relative frequency                             :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     Let $a_1, a_2, \ldots, a_k$ be the $k$ occurring messages in a
     communication, where a total of $n$ messages was transmitted.
     Let $m_j$ be the number of times $a_j$ occurs, \ie the
     *frequency* of $a_j$ during the communication, then $p_j =
     \frac{m_j}{n} > 0$ is the *relative frequency* of $a_j$.  
**** Distribution                                                  :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     The sequence $p_1, p_2, \ldots, p_k$ is the distribution if $0
     \le p_j \le 1$ and $\sum_{j=1}^k p_j = 1$.
*** Entropy
**** Self-information, unit of information                        :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     The *self-information* of the individual message $a_j$ is $I_j =
     -\log^r p_j$, where $1 < r \in \R$ is the *unit of
     information*. If $r=2$, then the unit of information is called a
     *bit*, or in the case of $r = e \approx 2.71$ it is called a
     *nat*.
**** Entropy                                                   :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     The expected value of self-information in a communication stream
      is the *entropy* $H_r(p_1, p_2, \ldots, p_k) = - \sum_{j=1}^k
      p_j \log_r p_j$.  The entropy depends only on the distribution
      $p_1, p_2, \ldots, p_k$ of messages, not their content.
*** The Jensen-inequality
**** Convex function
     Let $I \subset \R$ be an interval.  The $f : I \to \R$ function
     is a *convex function*, if for any $x_1, x_2 \in I$ and $0 \le t
     \le 1$, we have $f(t x_1 + (1 - t) x_2) \le t f(x_1) + (1 - t)
     f(x_2)$.  The function is strictly convex, if equality occurs
     only for $t=0$ and $t=1$.
**** Jensen-inequality                                            :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     Let $p_1, p_2, \ldots, p_k$ be a distribution, $f:I \to \R$ a
     strictly convex function (on the interval $I \subset \R$).  Then
     any for $q_1, q_2, \ldots, q_k \in I$, the $f(\sum_{j=1}^k p_j
     q_j) \le \sum_{j=1}^k p_j f(q_j)$, and equality occurs only if
     $q_1 = q_2 = \cdots = q_k$.
*** Upper bound of entropy
**** Upper bound of $H_r(p_1, p_2, \ldots, p_k)$                  :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     For any distribution $H_r(p_1, p_2, \ldots, p_k) \le \log_r k$,
     and equality only occurs if $p_1 = p_2 = \ldots = p_k =
     \frac{1}{k}$.
**** Proof                                                          :B_proof:
     :PROPERTIES:
     :BEAMER_env: proof
     :END:
     For $r > 1$ the $-\log_r(x)$ function is strictly convex, So by
     the Jensen-inequality and choosing $q_j =
     \frac{1}{p_j}$: 
     \begin{align*}
     -H_r(p_1, p_2, \ldots, p_k) &=
     \sum_{j=1}^k p_j \log_r p_j = \sum_{j=1}^k p_j\left(-\log_r \frac{1}{p_j} \right) \\
     &\ge -\log_r \left( \sum_{j=1}^k p_j \frac{1}{p_j} \right) =
     -\log_r k.
     \end{align*}
** Coding
*** Basics
**** Coding
     In the broadest sense, *coding* is modeled (in mathematics) as a
     map between two sets. If this map is
     invertible, the the coding is *lossless* or *invertible*, and
     *lossy* otherwise.  
**** Different fields of coding theory
     - *Data compression* or, *source coding*
     - *Error control* or *channel coding*
*** Source coding
**** Character encoding                                        :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     When the coding happens character-wise, then the coding is a
     *character encoding*.  More precisely the coding is a map $\psi :
     A^\ast \to B^\ast$, such that there is a character encoding
     $\varphi : A \to B^{\ast}$ and if $u = u_1 u_2 \cdots u_n \in
     A^\ast$, then $\psi(u) = \varphi(u_1) \varphi(u_2) \cdots
     \varphi(u_n)$. In this case, $A$ is the *character set*, $B$ is
     the *encoded character set* and $\rng(\varphi)$ is the set of
     *code-words*.  We assume that both $A$ and $B$ are finite.
**** Remarks                                                   :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     Usually we are interested in lossless encoding, and therefore
     $\varphi$ is always invertible and $\varphi : A \to B^+$.
     However this does not always guarantee that $\psi$ is also
     invertible: $\varphi(a) = 1$, $\varphi(b) = 01$, $\varphi(c) =
     10$, but $\psi(ab) = 101 = \psi(ca)$.

     Examples of character encoding: ASCII, UTF-8 (Unicode) etc.
*** Prefix
**** Prefix, suffix, infix                                     :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     Given a character set $A$, and words $\alpha, \beta, \gamma \in
     A^{\ast}$. Then $\alpha$ is a *prefix*, $\beta$ is an *infix* and
     $\gamma$ is a *suffix* of $\alpha \beta \gamma$.
**** Prefix-free set                                           :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     A set (of words) is *prefix-free* if it doesn't contain two
     different words, such that on is a prefix of the other.
**** Trivial and proper prefix, suffix and infix               :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     The empty string and $\alpha$ is a *trivial* prefix, infix and
     suffix of $\alpha$.  The empty string and a non-trivial prefix,
     infix or suffix of a word is a *proper* prefix, infix, suffix.
*** Different types of character encoding
**** Prefix, fixed-length, comma-separated codes               :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     Let $\varphi : A \to B^+$ injective map be a character encoding.  
     - If $\rng(\varphi)$ is prefix-free, then $\varphi$ is a *prefix code*.
     - If all elements of $\rng(\varphi)$ have the same length, then
       $\varphi$ is an *fixed-length* or *block code*.
     - If there is a $\vartheta \in B^+$ (a *comma*), such that
       $\vartheta$ is a suffix of every code-word, but none of the
       code-words can be written as $\alpha \theta \beta$ where
       $\beta$ is not the empty string, then $\varphi$ is a
       *comma-separated code*.
*** Property of prefix codes
**** Prefix codes are lossless                                    :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     Every prefix code is lossless, \ie it can be decoded on-the-fly.
**** Proof                                                          :B_proof:
     :PROPERTIES:
     :BEAMER_env: proof
     :END:
     Constructive: Start by reading the characters into a buffer until
     the characters in the buffer don't correspond to a
     code-word. When this happens, the read sub\-string can be decoded
     using $\varphi^{-1}$, because it is not prefix of any other
     code-word. The buffer can be discarded, and the process repeated
     on the rest of the unprocessed string.
*** Fixed-length and comma-separated codes are prefix codes
**** Fixed-length codes are prefix codes                          :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     Every fixed-length code is a prefix code.
**** Proof                                                          :B_proof:
     :PROPERTIES:
     :BEAMER_env: proof
     :END:
     Because code words are equal length, one code-word is a prefix of
     another code-word if they are the same.
**** Comma-separated codes are prefix codes                       :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     Every comma-separated code is a prefix code.
**** Proof                                                          :B_proof:
     :PROPERTIES:
     :BEAMER_env: proof
     :END:
     The comma marks the end of each code-word.  If a code-word would
     be a prefix of another code-word, then the comma would be a
     proper infix (in the second code-word).
*** Example
**** Character encoding                                           :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
     Let $A = \{a,b,c\}$ and $B = \{0, 1\}$, $\varphi : A \to B+$ as
     follows:
     |              |   1. | 2. |  3. | 4. | 5. |   6. |
     | $\varphi(a)$ |   01 |  1 |  01 |  0 | 00 |   01 |
     | $\varphi(b)$ | 1101 | 01 | 011 | 10 | 10 |  001 |
     | $\varphi(c)$ |   01 | 10 |  11 | 11 | 11 | 0001 |
     1. $\varphi(a) = \varphi(c)$: $\varphi$ is not injective;
     2. $\psi(ab) = 101 = \psi(ca)$: $\psi$ is lossy;
     3. Not prefix, but still lossless;
     4. Prefix code;
     5. Fixed-length code;
     6. Comma-separated code.
*** Kraft-McMillan inequality
**** Kraft-McMillan inequality                                    :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     Let $A = \{a_1, a_2, \ldots, a_n\}$ be the character set and $r
     \ge 2$ the number of elements of $B$, the encoding character and
     $\varphi : A \to B^+$ injective map.  If $\psi$ defined by
     $\varphi$ is lossless, and $\ell_j = \abs{ \varphi(a_j) }$ is the
     length of the code-word corresponding to $a_j$, then
     \[\sum_{j=1}^n r^{-\ell_j} \le 1.\]
**** Converse of Kraft-McMillan inequality
     Using the previous notation, if $\ell_1, \ell_2, \ldots \ell_n$
     are positive integers, such that $\sum_{j=1}^n r^{-\ell_j} \le
     1$, then there is a $\varphi : A \to B^+$ character encoding,
     /which is a prefix code/, such that the length of $\varphi(a_j)$
     is $\ell_j$.
*** Expected code-word length
**** Expected code-word length                                 :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     Let $A = \{a_1, a_2, \ldots, a_k \}$ be a character set with
     distribution $p_1, p_2, \ldots, p_k$, $\varphi: A \to B^+$
     injective map, and $\ell_j = \abs{\varphi(a_j)}$. Then
     $\overline{\ell} = \sum_{j=1}^k p_j \ell_j$ is the *expected
     code-word length*.
**** Optimal code                                              :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
     Given a character set and the corresponding distribution, if the
     expected code-word length is minimal, then the code is an
     *optimal code*.
**** Remark
     Since $\overline{\ell}$ is a real number, and subsets of real
     numbers don't always have a minimal number (\eg $\{1/n : n \in
     \N\}$), the existence of an optimal code is not a trivial
     question.
*** Existence of optimal codes
**** Existence of optimal codes                                   :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
     Given a character set and a distribution, there is always a
     coding which is optimal.
**** Proof                                                          :B_proof:
     :PROPERTIES:
     :BEAMER_env: proof
     :END:
     - Choose an arbitrary lossless encoding. (why can we select one?)
       Let $\ell$ be the expected code-word length of this code.
     - If $p_j \ell_j > \ell$, the code cannot be optimal (why?), so
       we can consider only codes with $\ell_j \le \frac{\ell}{p_j}$
       (for each $j = 1, 2, \ldots, k$).
     - There is a finite number of such codes (why?), and a finite
       subset of $\R$ has a minimal element.
* COMMENT  From hungarian slides
\begin{frame}[t]{Betűnkénti kódolás}
\begin{block}{Tétel (Shannon tétele zajmentes csatornára)}
Legyen $A=\{a_1,a_2,\ldots,a_n\}$ a kódolandó ábécé,
$p_1,p_2,\ldots,p_n$ a betűk eloszlása, $\varphi:A\to B^+$ injektív leképezés,
$B$ elemeinek a száma $r\ge 2$, továbbá $l_j=|\varphi(a_j)|$.\\
Ha a $\varphi$ által meghatározott betűnkénti kódolás felbontható,
akkor $H_r(p_1,p_2,\ldots,p_n)\le\overline{l}$.
\end{block}
\begin{block}{Bizonyítás}
$$\overline{l}-H_r(p_1,p_2,\ldots,p_n)=\sum_{j=1}^n p_jl_j+\sum_{j=1}^n p_j\log_rp_j=$$
$$=\sum_{j=1}^n p_j\cdot\left (-\log_r(r^{-l_j})\right )+\sum_{j=1}^n p_j\cdot\left (-\log_r\frac{1}{p_j}\right )=
\sum_{j=1}^n p_j\cdot\left (-\log_r\frac{r^{-l_j}}{p_j}\right )\ge$$
\small$$\ge-\log_r\left(\sum_{j=1}^nr^{-l_j}\right)\ge -\log_r1=0$$
\normalsize
\end{block}
\end{frame}

\begin{frame}[t]{Betűnkénti kódolás}
\begin{block}{Tétel (Shannon kód létezése)}
Az előző tétel jelöléseivel, ha $n>1$, akkor van olyan prefix kód, amire
$\overline{l}<H_r(p_1,p_2,\ldots,p_n)+1$.
\end{block}
\begin{block}{Bizonyítás}
Válasszunk olyan $l_1,l_2,\ldots,l_n$ természetes számokat,
amelyekre $r^{-l_j}\le p_j<r^{-l_j+1}$, ha $j=1,2,\ldots,n$ (Miért tudunk ilyeneket választani?).
Ekkor $\sum_{j=1}^nr^{-l_j}\le\sum_{j=1}^np_j=1$, így a McMillan-egyenlőtlenség megfordítása miatt létezik prefix kód az adott $l_j$ hosszakkal. Mivel $l_j<1-\log_rp_j$ (Miért?), ezért
$$\overline{l}=\sum_{j=1}^n p_jl_j<\sum_{j=1}^n p_j(1-\log_rp_j)=1+H_r(p_1,p_2,\ldots,p_n)\color{black}{.}$$
\end{block}
\end{frame}

\begin{frame}[t]{Optimális kódkonstrukció: Huffman-kód}
Legyen $\{a_1,a_2,\ldots,a_n\}$ az üzenetek halmaza, a hozzájuk tartozó eloszlás pedig 
$\{p_1,p_2,\ldots,p_n\}$, a kódábécé elemszáma $r$.\\
Rendezzük relatív gyakoriság szerint csökkenő sorrendbe a betűket.\\
Osszuk el maradékosan $n-2$-t $r-1$-gyel: $n-2=q(r-1)+m\quad 0\le m<r-1$, és legyen $t=m+2$.\\Helyettesítsük az utolsó $t$ betűt egy új betűvel, amihez az elhagyott betűk relatív gyakoriságainak összegét rendeljük,
és az így kapott gyakoriságoknak megfelelően helyezzük el az új betűt a sorozatban.\\
Ezek után ismételjük meg az előző redukciót, de most már minden lépésben $r$
betűvel csökkentve a kódolandó halmazt, mígnem már csak $r$ betű marad.\\
Most a redukált ábécé legfeljebb $r$ betűt tartalmaz, és ha volt redukció,
akkor pontosan $r$-et.\\
Ezeket a kódoló ábécé elemeivel kódoljuk,
majd a redukciónak megfelelően visszafelé haladva,
az összevont betűk kódját az összevonásként kapott betű már meglévő kódjának a kódoló ábécé különböző betűivel való kiegészítésével kapjuk.
\begin{block}{}
\end{block}
\end{frame}

\begin{frame}[t]{Példa Huffman-kódra}
Legyen $A=\{a,b,\ldots,j\}$, a relatív gyakoriságok
$0,17;0,02;0,13;0,02;0,01;0,31;0,02;0,17;0,06;0,09$, a kódoló ábécé pedig $\{0,1,2\}$.
$10-2=4\cdot(3-1)+0$, így $t=0+2=2$.\\
\vspace{0.3cm}
\scriptsize
\begin{tabular}{ccc}
\begin{tabular}{cl}
f & \ \ \ 0,31\\
a & \ \ \ 0,17\\
h & \ \ \ 0,17\\
c & \ \ \ 0,13\\
j & \ \ \ 0,09\\
i & \ \ \ 0,06\\
b & \ \ \ 0,02\\
d & \ \ \ 0,02\\
\begin{tabular}{c}
g\\
e
\end{tabular} &
$\color{black}{\left .\begin{tabular}{l}
0,02\\
0,01
\end{tabular}\right\}0,03}$
\end{tabular}

\begin{tabular}{cl}
f & \ \ \ 0,31\\
a & \ \ \ 0,17\\
h & \ \ \ 0,17\\
c & \ \ \ 0,13\\
j & \ \ \ 0,09\\
i & \ \ \ 0,06\\
\begin{tabular}{c}
(g,e)\\
b\\
d
\end{tabular} &
$\color{black}{\left .\begin{tabular}{l}
0,03\\
0,02\\
0,02
\end{tabular}\right\}0,07}$
\end{tabular}

\begin{tabular}{cl}
f & \ \ \ 0,31\\
a & \ \ \ 0,17\\
h & \ \ \ 0,17\\
c & \ \ \ 0,13\\
\begin{tabular}{c}
j\\
((g,e),b,d)\\
i
\end{tabular} &
$\color{black}{\left .\begin{tabular}{l}
0,09\\
0,07\\
0,06
\end{tabular}\right\}0,22}$
\end{tabular}

\end{tabular}\\
\vspace{0.3cm}
\begin{tabular}{cc}
\begin{tabular}{cl}
f & \ \ \ 0,31\\
(j,((g,e),b,d),i) & \ \ \ 0,22\\
\begin{tabular}{c}
a\\
h\\
c
\end{tabular} &
$\color{black}{\left .\begin{tabular}{l}
0,17\\
0,17\\
0,13
\end{tabular}\right\}0,47}$
\end{tabular}

\begin{tabular}{cl}
(a,h,c) & \ \ 0,47\\
f & \ \ 0,31\\
(j,((g,e),b,d),i) & \ \ 0,22
\end{tabular}

\end{tabular}
\begin{block}{}
\end{block}
\end{frame}

\begin{frame}[t]{Példa Huffman-kódra folyt.}
\begin{tabular}{cl}
(a,h,c) & \ \ 0,47\\
f & \ \ 0,31\\
(j,((g,e),b,d),i) & \ \ 0,22
\end{tabular}

\vspace{0.3cm}
Kódolás:\\
\hspace{0.5cm}(a,h,c)$\color{black}{\mapsto}$0\hspace{1.7cm}a$\color{black}{\mapsto}$00\\
\hspace{3.7cm}h$\color{black}{\mapsto}$01\\
\hspace{3.7cm}c$\color{black}{\mapsto}$02\\
\hspace{1cm}f$\color{black}{\mapsto}$1\\
(j,((g,e),b,d),i)$\color{black}{\mapsto}$2\hspace{1cm}j$\color{black}{\mapsto}$20\\
\hspace{3.1cm}((g,e),b,d)$\color{black}{\mapsto}$21\hspace{1cm}(g,e)$\color{black}{\mapsto}$210
\hspace{1cm}g$\color{black}{\mapsto}$2100\\
\hspace{9.05cm}e$\color{black}{\mapsto}$2101\\
\hspace{6.8cm}b$\color{black}{\mapsto}$211\\
\hspace{6.8cm}d$\color{black}{\mapsto}$212\\
\hspace{3.7cm}i$\color{black}{\mapsto}$22\\
Entrópia: $\approx 1,73$.\\
Átlagos szóhossz: $1,79.$
\end{frame}

\begin{frame}[t]{Betűnkénti kódolás}
\begin{block}{Tétel (NB)}
A Huffman-kód optimális.
\end{block}
\begin{block}{Példa Shannon-kódra}
Az előző példában használt ábécét és eloszlást fogjuk használni. Rendezzük sorba
az ábécét relatív gyakoriságok szerinti csökkenő sorrendben:\\
\begin{tabular}{cl}
f & 0,31\\
a & 0,17\\
h & 0,17\\
c & 0,13\\
j & 0,09\\
i & 0,06\\
b & 0,02\\
d & 0,02\\
g & 0,02\\
e & 0,01
\end{tabular}
\end{block}
\end{frame}

\begin{frame}[t]{Példa Shannon-kódra folyt.}
Határozzuk meg a szükséges szóhosszúságokat:
$\frac{1}{9}\le 0,31;0,17;0,13<\frac{1}{3}$, ezért f, a, h és c kódhossza 2.\\
$\frac{1}{27}\le 0,09;0,06<\frac{1}{9}$, ezért j és i kódhossza 3.\\
$\frac{1}{81}\le 0,02<\frac{1}{27}$, ezért b, d és g kódhossza 4.\\
$\frac{1}{243}\le 0,01<\frac{1}{81}$, ezért e kódhossza 5.\\
Az f kódja 00, az a kódja 01, a h kódja 02, és ez utóbbihoz 1-et adva hármas alapú számrendszerben kapjuk c kódját, ami 10. Ehhez 1-et adva 11-et kapunk, de j kódjának hossza 3, ezért ezt még ki kell egészíteni jobbról egy 0-val, tehát j kódja 110. Hasonlóan folytatva
megkapjuk a teljes kódot:\\
\scriptsize

\begin{tabular}{cl}
f & 00\\
a & 01\\
h & 02\\
c & 10\\
j & 110\\
i & 111\\
b & 1120\\
d & 1121\\
g & 1122\\
e & 12000
\end{tabular}\\
\normalsize
Átlagos szóhossz: $2,3<1,73+1$.
\end{frame}

\begin{frame}[t]{Betűnkénti kódolás}
\small
\begin{block}{Kódfa}
A betűnkénti kódolás szemléltethető egy címkézett irányított fával.\\
Legyen $\varphi:A\to B^*$ egy betűnkénti kódolás, és tekintsük $\rng(\varphi)$ prefixeinek halmazát. Ez a halmaz részbenrendezett a ,,prefixe'' relációra. Vegyük ennek a Hasse-diagramját. Így egy irányított fát kapunk, aminek a gyökere az üres szó,
és minden szó a hosszának megfelelő szinten van.\\
A fa éleit címkézzük úgy $B$ elemeivel, hogy ha $\beta=\alpha b$ valamely $b\in B$-re,
akkor az $\alpha$-ból $\beta$-ba vezető él címkéje legyen $b$.\\
A kódfa csúcsait is megcímkézhetjük: az $a\in A$ kódjának megfelelő csúcs címkéje legyen $a\in A$; azon csúcs címkéje, amely nincsen $\rng(\varphi)$-ben, legyen ,,üres''.
\end{block}
\footnotesize 
\begin{block}{Megjegyzés}
Az előbbi konstrukció meg is fordítható. Tekintsünk egy véges, élcímkézett irányított fát, ahol az élcímkék halmaza $B$, az egy csúcsból kiinduló élek mind különböző címkéjűek,
továbbá az $A$ véges ábécének a csúcsokra való leképezését, amelynél minden levél előáll képként.\\
Az $a\in A$ betű kódja legyen az a szó, amelyet úgy kapunk,
hogy a gyökértől az $a$-nak megfelelő csúcsig haladó irányított út mentén összeolvassuk az élek címkéit.
\end{block}
\end{frame}

\begin{frame}[t]{Kódfa}
\begin{block}{Példa}
\begin{center}
\begin{tikzpicture}
\filldraw [black] (0,0) circle (1pt)
		 (-3,-1) circle (1pt)
		 (0,-1) circle (1pt)
		 (3,-1) circle (1pt)
		 (-4.5,-2) circle (1pt)
		 (-3,-2) circle (1pt)
		 (-1.5,-2) circle (1pt)
		 (1.5,-2) circle (1pt)
		 (3,-2) circle (1pt)
		 (4.5,-2) circle (1pt)
		 (1.8,-3) circle (1pt)
		 (3,-3) circle (1pt)
		 (4.2,-3) circle (1pt)
		 (1.2,-4) circle (1pt)
		 (1.8,-4) circle (1pt);
		
\draw [->, thin,>=latex] (0,0) -- (-3,-1);
\draw [->, thin,>=latex] (0,0) -- (0,-1);
\draw [->, thin,>=latex] (0,0) -- (3,-1);
\draw [<-, thin,>=latex] (-4.5,-2) -- (-3,-1);
\draw [<-, thin,>=latex] (-3,-2) -- (-3,-1);
\draw [<-, thin,>=latex] (1.5,-2) -- (3,-1);
\draw [<-, thin,>=latex] (4.5,-2) -- (3,-1);
\draw [<-, thin,>=latex] (3,-2) -- (3,-1);
\draw [<-, thin,>=latex] (-1.5,-2) -- (-3,-1);
\draw [->, thin,>=latex] (3,-2) -- (1.8,-3);
\draw [->, thin,>=latex] (3,-2) -- (3,-3);
\draw [->, thin,>=latex] (3,-2) -- (4.2,-3);
\draw [->, thin,>=latex] (1.8,-3) -- (1.2,-4);
\draw [->, thin,>=latex] (1.8,-3) -- (1.8,-4);

\draw (-1.5,-0.3) node[scale=0.8] {0};
\draw (-0.1,-0.4) node[scale=0.8] {1};
\draw (1.5,-0.3) node[scale=0.8] {2};
\draw (0,-1.2) node[scale=0.8] {f};
\draw (-3.8,-1.3) node[scale=0.8] {0};
\draw (-3.1,-1.5) node[scale=0.8] {1};
\draw (-2.2,-1.3) node[scale=0.8] {2};
\draw (-4.5,-2.2) node[scale=0.8] {a};
\draw (-3,-2.2) node[scale=0.8] {h};
\draw (-1.5,-2.2) node[scale=0.8] {c};
\draw (3.8,-1.3) node[scale=0.8] {2};
\draw (2.9,-1.5) node[scale=0.8] {1};
\draw (2.2,-1.3) node[scale=0.8] {0};
\draw (4.5,-2.2) node[scale=0.8] {i};
\draw (1.5,-2.2) node[scale=0.8] {j};
\draw (3.6,-2.3) node[scale=0.8] {2};
\draw (2.9,-2.5) node[scale=0.8] {1};
\draw (2.4,-2.3) node[scale=0.8] {0};
\draw (4.2,-3.2) node[scale=0.8] {d};
\draw (3,-3.2) node[scale=0.8] {b};
\draw (1.7,-3.5) node[scale=0.8] {1};
\draw (1.4,-3.4) node[scale=0.8] {0};
\draw (1.2,-4.2) node[scale=0.8] {g};
\draw (1.8,-4.2) node[scale=0.8] {e};

\end{tikzpicture}
\end{center}
A Huffman-kódos példában szereplő kódhoz tartozó kódfa.\\
$\varphi(a)=$00, $\varphi(b)=$211, $\varphi(c)=$02, $\varphi(d)=$212, $\varphi(e)=$2101,
$\varphi(f)=$1, $\varphi(g)=$2100, $\varphi(h)=$01, $\varphi(i)=$22, $\varphi(j)=$20.\\
A kódszavak prefixeinek halmaza:
$\{\lambda,1,00,0,01,02,20,2,22,211,21,212,2100,210,2101\}$
\end{block}
\end{frame}

\begin{frame}[t]{Kódfa}
\begin{block}{Példa}
\begin{center}


\resizebox{!}{4.4cm}{

\begin{tikzpicture}
\filldraw [black] (0,0) circle (1pt)
		 (-3,-1) circle (1pt)
		 (0,-1) circle (1pt)
%		 (3,-1) circle (1pt)
		 (-4.5,-2) circle (1pt)
		 (-3,-2) circle (1pt)
		 (-1.5,-2) circle (1pt)
		 (-1,-2) circle (1pt)
		 (0,-2) circle (1pt)
		 (3.2,-2) circle (1pt)
%		 (3,-2) circle (1pt)
%		 (4.5,-2) circle (1pt)
		 (-0.8,-3) circle (1pt)
		 (0,-3) circle (1pt)
		 (0.8,-3) circle (1pt)
		 (2.4,-3) circle (1pt)
%		 (4.2,-3) circle (1pt)
		 (0.2,-4) circle (1pt)
		 (0.8,-4) circle (1pt)
     (1.4,-4) circle (1pt)
     (1.8,-4) circle (1pt)
     (1.5,-5) circle (1pt);
		
\draw [->, thin,>=latex] (0,0) -- (-3,-1);
\draw [->, thin,>=latex] (0,0) -- (0,-1);
%\draw [->, thin,>=latex] (0,0) -- (3,-1);
\draw [<-, thin,>=latex] (-4.5,-2) -- (-3,-1);
\draw [<-, thin,>=latex] (-3,-2) -- (-3,-1);
%\draw [<-, thin,>=latex] (1.5,-2) -- (3,-1);
%\draw [<-, thin,>=latex] (4.5,-2) -- (3,-1);
%\draw [<-, thin,>=latex] (3,-2) -- (3,-1);
\draw [<-, thin,>=latex] (-1.5,-2) -- (-3,-1);
%\draw [->, thin,>=latex] (3,-2) -- (1.8,-3);
%\draw [->, thin,>=latex] (3,-2) -- (3,-3);
%\draw [->, thin,>=latex] (3,-2) -- (4.2,-3);
%\draw [->, thin,>=latex] (1.8,-3) -- (1.2,-4);
%\draw [->, thin,>=latex] (1.8,-3) -- (1.8,-4);

\draw [->, thin,>=latex] (0,-1) -- (0,-2);
\draw [->, thin,>=latex] (0,-1) -- (-1,-2);
\draw [->, thin,>=latex] (0,-2) -- (-0.8,-3);
\draw [->, thin,>=latex] (0,-2) -- (0,-3);
\draw [->, thin,>=latex] (0,-2) -- (0.8,-3);
\draw [->, thin,>=latex] (0.8,-3) -- (0.2,-4);
\draw [->, thin,>=latex] (0.8,-3) -- (0.8,-4);
\draw [->, thin,>=latex] (0.8,-3) -- (1.4,-4);

\draw [->, thin,>=latex] (0,-1) -- (3.2,-2);
\draw [->, thin,>=latex] (3.2,-2) -- (2.4,-3);
\draw [->, thin,>=latex] (2.4,-3) -- (1.8,-4);
\draw [->, thin,>=latex] (1.8,-4) -- (1.5,-5);

\draw (-1.5,-0.3) node[scale=0.8] {0};
\draw (-0.1,-0.4) node[scale=0.8] {1};
%\draw (1.5,-0.3) node[scale=0.8] {2};
%\draw (0,-1.2) node[scale=0.8] {f};
\draw (-3.8,-1.3) node[scale=0.8] {0};
\draw (-3.1,-1.5) node[scale=0.8] {1};
\draw (-2.2,-1.3) node[scale=0.8] {2};
\draw (-4.5,-2.2) node[scale=0.8] {f};
\draw (-3,-2.2) node[scale=0.8] {a};
\draw (-1.5,-2.2) node[scale=0.8] {h};
\draw (-1,-2.2) node[scale=0.8] {c};
\draw (1.6,-1.3) node[scale=0.8] {2};
\draw (-0.1,-1.5) node[scale=0.8] {1};
\draw (-0.5,-1.3) node[scale=0.8] {0};
\draw (0,-3.2) node[scale=0.8] {i};
\draw (-0.8,-3.2) node[scale=0.8] {j};
\draw (0.4,-2.3) node[scale=0.8] {2};
\draw (-0.1,-2.5) node[scale=0.8] {1};
\draw (2.7,-2.4) node[scale=0.8] {0};
\draw (-0.4,-2.3) node[scale=0.8] {0};
\draw (0.8,-4.2) node[scale=0.8] {d};
\draw (0.2,-4.2) node[scale=0.8] {b};
\draw (0.7,-3.5) node[scale=0.8] {1};
\draw (0.4,-3.4) node[scale=0.8] {0};
\draw (1.2,-3.4) node[scale=0.8] {2};
\draw (2,-3.4) node[scale=0.8] {0};
\draw (1.4,-4.2) node[scale=0.8] {g};
\draw (1.5,-5.2) node[scale=0.8] {e};
\draw (1.55,-4.5) node[scale=0.8] {0};

\end{tikzpicture}


}


\end{center}
A Shannon-kódos példában szereplő kódhoz tartozó kódfa.\\
$\varphi(a)=$01, $\varphi(b)=$1120, $\varphi(c)=$10, $\varphi(d)=$1121, $\varphi(e)=$12000,
$\varphi(f)=$00, $\varphi(g)=$1122, $\varphi(h)=$02, $\varphi(i)=$111, $\varphi(j)=$110.\\
A kódszavak prefixeinek halmaza:
\small$\{01,0,\lambda,1120,112,11,1,10,1121,12000,1200,120,12,00,1122,02,111,110\}$
\end{block}
\end{frame}
# "~/Dropbox/dm2gabor/eloadas/dm2C/dm2_eaC_08_17tav.tex"
